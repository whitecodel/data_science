{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Mastering Feature Engineering with the Titanic Dataset\n", "In this notebook, we will go through the complete feature engineering process using the Titanic dataset, including handling missing values, encoding, creating new features, feature scaling, feature selection, and data visualization."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Importing Necessary Libraries\n", "We start by importing the necessary libraries for data manipulation, visualization, and machine learning."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Import necessary libraries\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.ensemble import RandomForestClassifier\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Loading the Titanic Dataset\n", "Let's load the Titanic dataset directly from the provided URL. We'll also display the first few rows to get an understanding of the dataset structure."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load Titanic dataset\n", "df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n", "\n", "# Display the first few rows of the dataset\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Data Preprocessing\n", "We will drop irrelevant columns that do not contribute to predicting the target variable (Survived)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Drop irrelevant columns\n", "df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n", "\n", "# Display the dataset after dropping columns\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Handling Missing Values\n", "Handling missing values is crucial for maintaining data integrity. We'll fill missing values for the 'Age' and 'Embarked' columns."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fill missing values in 'Age' with the mean age\n", "df['Age'] = df['Age'].fillna(df['Age'].mean())\n", "\n", "# Fill missing values in 'Embarked' with the most common port\n", "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n", "\n", "# Check for any remaining missing values\n", "df.isnull().sum()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Encoding Categorical Variables\n", "We need to convert categorical variables into a numerical format that machine learning models can understand."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Encoding 'Sex' and 'Embarked' columns\n", "df['Sex'] = df['Sex'].map({'female': 0, 'male': 1})\n", "df['Embarked'] = df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n", "\n", "# Display the dataset after encoding\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Creating New Features\n", "Creating new features can help improve model performance. We'll create two new features: `FamilySize` and `IsAlone`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create a new feature 'FamilySize'\n", "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n", "\n", "# Create a new feature 'IsAlone'\n", "df['IsAlone'] = 1  # Initialize to 1 (Alone)\n", "df.loc[df['FamilySize'] > 1, 'IsAlone'] = 0  # Set to 0 if not alone\n", "\n", "# Display the dataset with new features\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. Feature Scaling\n", "Feature scaling ensures that numerical features are on the same scale. We will standardize the `Age` and `Fare` columns."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Standardize the 'Age' and 'Fare' columns\n", "scaler = StandardScaler()\n", "df[['Age', 'Fare']] = scaler.fit_transform(df[['Age', 'Fare']])\n", "\n", "# Display the dataset after scaling\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8. Feature Selection\n", "We will use a Random Forest model to identify the most important features for predicting survival."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define the features and target variable\n", "X = df.drop('Survived', axis=1)\n", "y = df['Survived']\n", "\n", "# Train a Random Forest model\n", "model = RandomForestClassifier()\n", "model.fit(X, y)\n", "\n", "# Get feature importances\n", "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n", "feature_importances.sort_values(ascending=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9. Data Visualization\n", "Visualizing data helps in understanding patterns and relationships. We will create a few plots to visualize the dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Correlation Matrix\n", "plt.figure(figsize=(10, 8))\n", "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n", "plt.title('Correlation Matrix')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Age Distribution by Survival Status\n", "plt.figure(figsize=(10, 6))\n", "sns.histplot(data=df, x='Age', hue='Survived', multiple='stack')\n", "plt.title('Age Distribution by Survival Status')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Family Size vs. Survival Rate\n", "plt.figure(figsize=(10, 6))\n", "sns.barplot(data=df, x='FamilySize', y='Survived', ci=None)\n", "plt.title('Family Size vs. Survival Rate')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 10. Saving the Processed Dataset\n", "We will save the processed dataset for future use in model training and evaluation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Save the processed dataset\n", "df.to_csv('titanic_processed.csv', index=False)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.7.0"}}, "nbformat": 4, "nbformat_minor": 4}